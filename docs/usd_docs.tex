\documentclass[12pt,a4paper]{article}

% for polish language
\usepackage{polski}

% for some math symbols
\usepackage{amssymb}

% correct footnotes placement
\usepackage[bottom]{footmisc}

% for \say command
\usepackage{dirtytalk}

% change title of the bibliography
\def\bibname{Referencje}\let\refname\bibname

% for advanced math formulas
\usepackage{mathtools}

\usepackage{listings}

% links
\usepackage{hyperref}
% \hypersetup{
%     colorlinks,
%     citecolor=black,
%     filecolor=black,
%     linkcolor=black,
%     urlcolor=blue
% }

% image displaying
\usepackage{subcaption}
\usepackage{graphicx}

\usepackage{multirow} 
\usepackage{makecell}

\title{Dokumentacja do projektu z USD}

\author{Roman Moskalenko \and Pavel Tarashkevich}
\date{}

\begin{document}

\maketitle

\section{Treść zadania}

\textbf{LL1.AnyTrading Stocks}\\

Zapoznaj się z \href{https://github.com/AminHP/gym-anytrading}{gym-anytrading} oraz
zbiorem danych tam dostępnym:

LL1 - STOCKS\_GOOGL

Przygotuj agenta podejmującego akcje na rynku i uczącego się ze wzmocnieniem
oraz porównaj jego skuteczność ze strategią losową. Przetestuj co najmniej 2 różne
algorytmy uczenia ze wzmocnieniem. Przygotuj skrypt umożliwiający wizualizację
działań wytrenowanego agenta.

\section{Projekt wstępny}

\subsection{Założenia ogólne}

Dla zaimplementowania agenta zajmującego się handlowaniem na rynku mamy
posłużyć algorytmem uczenia się ze wzmocnieniem (dalej RL). Jako założenie przyjmujemy,
że środowisko agenta, którym posłużymy się, będzie tylko częściowo odwzorowywać
rzeczywiste środowisko rynku. Będzie ono zawierało minimalny zestaw
atrybutów, niezbędnych do nauczenia agenta oraz najprostsze akcje
(\emph{kup}/\emph{sprzedaj}) do dyspozycji agenta.

Zakładamy, że algorytmy RL sprawdzą się lepiej, niż strategia losowa.

\subsection{Krótki opis przydzielonego środowiska}

W ramach projektu będziemy używać środowiska do symulacji handlowania
na rynku akcji podane w treści zadania: \href{https://github.com/AminHP/gym-anytrading}{gym-anytrading}.
Dane środowisko jest pochodną popularnego środowiska do trenowania agentów
RL \href{https://github.com/openai/gym}{gym} i zawiera ono podstawowe
jego funkcje. Zawiera ono także minimalny zestaw atrybutów opisujących
rynek akcji. Ono zawiera specjalną instancję \mbox{\emph{StockEnv}},
która naszym zdaniem najlepiej odwzorowuje środowisko dla naszego
zadania.

\subsection{Wybrane algorytmy}

Do realizacji zadania wybraliśmy algorytmy A2C, PPO. Literatura dotycząca
stosowania algorytmów RL dla trenowania agentów do handlowania na rynku
akcji wskazuje, że te dwa algorytmy dobrze się sprawdzają
\cite{ensemble_strat}.

Algorytm A2C (\emph{Advantage Actor-Critic}) bazuje na pomyśle
wykorzystania sieci neuronowych do aproksymacji polityki agenta (aktor)
oraz do aproksymacji funkcji wartości (krytyk).

Algorytm PPO (\emph{Proximal Policy Optimization}) rozwija algorytm A2C,
wykorzystując specjalne metody do optymalizacji polityki agenta.

\medskip

Porównamy je z algorytmem losowym.

\subsection{Biblioteki wybrane do realizacji projektu}

\begin{itemize}
  \item \textbf{gym-anytrading} - środowisko do trenowania agentów RL do
        handlowania na rynku akcji.
  \item \textbf{\href{https://github.com/notadamking/Stock-Trading-Visualization}{Stock-Trading-Visualization}} -
        skrypt do wizualizacji działań agenta oparty o bibliotekę \emph{matplotlib}.
  \item \textbf{\href{https://github.com/DLR-RM/stable-baselines3}{stable-baselines}} -
        biblioteka zawierająca gotowe implementację algorytmów RL.
\end{itemize}

\subsection{Propozycja eksperymentów do przeprowadzenia}\label{exps_proposal}

Zamierzamy przeprowadzić trenowanie wybranych modeli stosując różne
podejścia wyboru podzbioru danych uczących - danych z różnych okresów
czasowych.

Nie jest oczywiste jak powinna wyglądać strategia nagradzania agenta:
warto uwzględnić ten fakt, że w handlowaniu na rynku może być większa
preferencja natychmiastowego zysku, aby móc go potem wykorzystać w
późniejszych akcjach agenta. Uwzględnimy to, dostosowując parametr
dyskontowania nagród.

W celu ewaluacji działania algorytmów zastosujemy następujące metryki
m.i. wykorzystane w \cite{ensemble_strat}: \emph{Cumulative return},
\emph{Annualized return}, \emph{Annualized volatility}, \emph{Sharpe ratio},
\emph{Max drawdown}.

\pagebreak
\section{Projekt końcowy}

\subsection{Uzupełnienie projektu wstępnego}

Niestety postanowiliśmy zrezygnować z korzystania ze skryptów \textbf{Stock-Trading-Visualization},
ponieważ opierają się na innej bibliotece do handlowania na rynku przez
agentów RL i jest niekompatybilny z \textbf{gym-anytrading}.

W przykładach użycia biblioteki \textbf{gym-anytrading} zauważyliśmy
użycie biblioteki
\textbf{\href{https://github.com/ranaroussi/quantstats}{quantstats}}
do analizy operacji finansowych. Postanowiliśmy ją wykorzystać, gdyż
pozwala wyznaczyć metryki wspomniane przez nas w rozdziale
\ref{exps_proposal}.

\subsection{Uczenie agentów}

Dane dostępne z biblioteki \textbf{gym-anytrading}, które zostały wykorzystane
w ramach projektu podzieliliśmy na dwa podzbiory: dane uczące i dane testowe.
Podział został dokonany w taki sposób, że pierwsze $80\%$ danych tworzą
zbiór uczący, a reszta - zbiór testowy. \\

Z powodów oczywistych agent wykorzystujący politykę losową nie był uczony.
\smallskip

Wybraliśmy domyślne konfiguracje algorytmów A2C i PPO.
Ich uczenie dokonywano w ciągu 1,000,000 kroków,
gdzie za krok uznajemy pojedynczą akcją dokonaną przez
agenta na danym środowisku. Uczenie przeprowadziliśmy dla różnych zestawów
parametrów \emph{rozmiaru okna} - zdarzeń historycznych dostępnych agentowi
w każdym kroku działania (10, 20, 30) oraz \emph{dyskonta} (0.7, 0.9, 0.99).

Uczenie jest dokonane łącznie 3 razy
dla każdego algorytmu oraz w celu zmniejszenia wpływu
losowości na uzyskane wyniki.

Na rysunku \ref{fig:training} pokazano przebieg uczenia agentów.
Jak widać sumy nagród uzyskiwane przez agentów dla parametru dyskonta
równego 0.7 są wyraźnie mniejsze w porównaniu z 0.9 i 0.99. Także widać, że
rozmiar okna czasowego nie wiele zmienia pod względem efektywności uczenia.
Charakterystycznym jest ten fakt, że algorytm PPO pokazuje lepszą wydajność,
niż A2C, czego można było po nim się spodziewać.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\textwidth]{../src/plot0.pdf}
  \caption{Przebieg uczenia agentów w przeciągu 1,000,000 kroków.
    Na wykresie pokazano zakres sum nagród, uzyskany przez
    średnią sumę nagród oraz odchylenia standardowego.}
  \label{fig:training}
\end{figure}

\subsection{Ewaluacja działania agentów}


\pagebreak
\begin{thebibliography}{9}
  \bibitem{ensemble_strat}
  Yang, Hongyang and Liu, Xiao-Yang and Zhong, Shan and Walid, Anwar,
  Deep Reinforcement Learning for Automated Stock Trading: An Ensemble
  Strategy (September 11, 2020). Available at SSRN:
  \href{https://ssrn.com/abstract=3690996}{https://ssrn.com/abstract=3690996}
  or \href{http://dx.doi.org/10.2139/ssrn.3690996}{http://dx.doi.org/10.2139/ssrn.3690996}
\end{thebibliography}

\end{document}
